---
output:
  html_document:
    title: "intro bayes"
    highlight: pygments

---

# Análisis Bayesiano

Los análisis Bayesianos son similares a los que vimos en [máxima verosimilitud](http://rpubs.com/pajaro/mle) en el sentido que dependen explícitamente de modelos probabilísticos para los datos. Pero la gran diferencia es que con Bayes, obtenemos distribuciones de probabilidades para todas las cantidades no observadas, incluyendo parámetros, valores perdidos o nuevas observaciones. De esta manera, los análisis Bayesianos nos permiten **cuantificar incertidumbre**. La regla de Bayes planteada en términos de datos y parámetros es:

$$
p(\boldsymbol{\theta} \lvert \boldsymbol{y}) = \frac{p(\boldsymbol{y} \lvert \boldsymbol{\theta}) p(\boldsymbol{\theta)}}{\int p(\boldsymbol{y} \lvert \boldsymbol{\theta)} d \boldsymbol{\theta} }
$$

Es decir que la probabilidad *posterior* de los parámetros $\boldsymbol{\theta}$ dado que observamos los datos $\boldsymbol{y}$ es igual al likelihood por las previas dividido la probabilidad de los datos.

producto de la probabilidad de observar los datos condicional al valore de los parámetros ( $p(\boldsymbol{y} \lvert \boldsymbol{\theta})$ )  

Como veremos más adelante, los análisis Bayesianos combinados con métodos numéricos permiten analizar modelos con muchos parámetros y niveles de variabilidad pero primero vamos a empezar por casos simples donde podemos calcular las posteriores directamente. 

```{r}
url <- "https://github.com/jmmorales/cursoMyD/raw/master/Data/quintral.txt"
quintral <- read.table(url, header = TRUE)
# Son datos de remociÛn de frutos de sitios pareados con bosque continuo y fragmentado 

```


