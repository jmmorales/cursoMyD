---
title: "MLE"
author: "[Juan Manuel Morales](https://sites.google.com/site/pajarom/) "
date: "`r Sys.Date()`"
fontsize: 12pt
output: pdf_document
---

```{r, echo=FALSE}
library(knitr)
options(figure_counter = FALSE)
opts_knit$set(eval.after='fig.cap')
knitr::opts_chunk$set(fig.width=5, fig.height=4, tidy=TRUE, results='markup',
                      echo=TRUE, warning=FALSE, message=FALSE)
```


#   Máxima Verosimilitud (Likelihood) 

## objetivos: 
 - Relacionar el "likelihood" de datos con parámetros de una distribución 
 - Familiarizarse con curvas de likelihood y log-likelihood 
 - Comprobar que la forma de la curva de likelihood depende del tamaño muestreal
 - Aprender a usar herramientas para encontrar óptimos

Vimos que las [funciones de densidad](https://es.wikipedia.org/wiki/Función_de_densidad_de_probabilidad) nos dan la probabilidad de observar un valor determinado de una variable aleatoria (para variables continuas nos dan la probabilidad alrededor de un valor). Por ejemplo, para una distribución de Poisson, la probabilidad para un valor $y$ es $\lambda^{y} \exp(- \lambda) / y!$ (donde $!$ es el [factorial](https://es.wikipedia.org/wiki/Factorial) de $y$, no que $y$ sea copado...). Entonces, para una variable aleatoria que asumimos tiene una distribución de Poisson con $\lambda=2$ la probabilidad de observar un cero ($y=0$) es de $2^{0} \exp(-2)/0! = \exp(-2)$. En `R` usamos la función `dpois` para hacer este tipo de cuentas. Para este ejemplo escribimos `dpois(0, lambda=2)`. 

Normalmente tenemos un conjunto de datos, por ejemplo números de renovales por metro cuadrado de muestreo en un bosque: $y = 1, 0, 2, 1, 0, 0, 1, 1, 2, 2$. Si asumimos que estos datos se pueden describir como el resultado de un proceso aleatorio de tipo Poisson, y que además **las observaciones son independientes**, podemos calcular la probabilidad del conjunto de datos como el producto de las probabilidades de las observaciones:

$$
p(y)= \prod_i^n{ \frac{\lambda^{y_i} \exp(- \lambda)} {y_i!} }
$$

Podemos ver que el producto ($\prod$) se hace usándo $i$ como índice para cada observación del set de datos $y$,  desde $i=1$ hasta $i=n$ que es el tamaño de $y$. Esta probabilidad de observar los datos es lo que llamamos *likelihood* o *verosimilitud*. Para hacer la cuenta de arriba tenemos que definir además el parámetro $\lambda$. La probabilidad de los datos depende entonces no sólo de los valores presentes en el set de datos sino también de la distribución utilizada y el valor del o los parámetros de la distribución. De manera más general, la expresión es un **modelo de datos**, es decir una descripción probabilística de como llegar a los datos. Más adelante veremos como combinando procesos determinísticos y estocásticos podemos armar modelos de datos realistas y satisfactorios.

\newpage
Veamos ahora como calcular la probabilidad de estos datos en `R` asumiendo $\lambda = 1$:

```{r, tidy=TRUE}
y <- c(1, 0, 2, 1, 0, 0, 1, 1, 2, 2)
lambda <- 1
p.y <- dpois(y, lambda=lambda)
p.y 
prod(p.y)
```

Como las probabilidades son números que van entre $0$ y $1$, el producto de varias probabilidades resulta en números muy pequeños. Vemos en este caso que la probabilidad del **conjunto** de datos es muy baja (`5.67e-06`,  donde `e-06` es $10^{-6}$ o $0.000001$) pero esto no debería sorprendernos porque esa es la probabilidad de encontrar exactamente esos datos. Entonces, para evitar problemas numéricos preferimos trabajar con la suma de los logaritmos de las probabilidades, es decir con el logaritmo del *likelihood*. En `R`, podemos usar la opción `log = TRUE` dentro de las funciones de densidad.  

```{r}
y <- c(1, 0, 2, 1, 0, 0, 1, 1, 2, 2)
lambda <- 1
ll.y <- dpois(y, lambda=lambda, log = TRUE)
sum(ll.y)
```

Una vez que logramos hacer estas cuentas, podemos preguntarnos cómo cambia la probabilidad de observar el set de datos si cambiamos el valor de $\lambda$. Probemos con $\lambda=2$

```{r}
y <- c(1, 0, 2, 1, 0, 0, 1, 1, 2, 2)
lambda <- 2
sum(dpois(y, lambda=lambda, log = TRUE))
```

Con un $\lambda$ de $2$ la *log likelihood* de los datos empeora, pero podemos seguir probando distintos valores de $\lambda$ hasta encontrar el que hace más posible al set de datos. Esta es la idea detrás de los análisis de máxima verosimilitud (a falta de mejor nombre) o de *maximum likelihood* (si estamos dispuestos a usar palabras en inglés). Normalmente tenemos más de un parámetro en nuestro modelo de datos, y usamos aglún algoritmo de búsqueda para encontrar la combinación de parámetros que hace más probables a nuestros datos (por tradición, los algoritmos buscan minimizar el menos logaritmo de la verosimilitud de nuestros datos). Esta idea de buscar combinaciones de parámetros que maximizan la verosimilitud de los datos se conoce como *maximum likelihood estimation* y muchas veces aparece abreviado como MLE.

## Ejemplo: Remoción de Frutos

Mariano Rodriguez-Cabal estudió la remoción de frutos de quintral por parte del monito del monte en bosque continuo y fragmentos en 3 sitios pareados con la idea de ver si la fragmentación afectabla la interacción entre este marsupial y el muérdago. El trabajo  está dispoible [aquí](http://sites.google.com/site/modelosydatos/Rodriguez-Cabal_07.pdf)
 
Los datos son de número de frutos removidos de un total disponible. Podemos usar una distribución Binomial para capturar la variabilidad (estocasticidad) en el proceso de remoción de frutos. Es decir, asumimos que la distribución Binomial tiene sentido para representar el proceso que genera los datos.
 
```{r}

url <- "https://github.com/jmmorales/cursoMyD/raw/master/Data/quintral.txt"
quintral <- read.table(url, header = TRUE)

```

Para ver como están organizados los datos podemos hacer:
```{r}
str(quintral)  

```
Vemos que se trata de un *data frame* que es la manera preferida de `R` para organizar datos. Los data frames tienen a las observaciones en filas y a las variables en columnas. Las variables individuales se pueden acceder escribiendo el nombre del data frame seguido del signo $ y el nombre de la variable. Por ejemplo, si queremos ver los valores de frutos disponibles en las tres primeras observaciones hacemos:

```{r}
quintral$Frutos[1:3]
```

Ahora calculemos el likelihood para la primera observación del set de datos asumiendo que la probabilidad de remoción es $0.5$ (si no nos acordamos cómo es la distribución Binomial o cómo usarla en `R` podemos pedir ayuda con `?dbinom`)

```{r}
dbinom(quintral$Removidos[1],size=quintral$Frutos[1],prob=0.5)
```

Normalmente trabajamos con log-likelihoods:
```{r}
dbinom(quintral$Removidos[1],size=quintral$Frutos[1],prob=0.5,log=TRUE)
```

¿Cómo cambia la probabilidad de observar ese dato cuando cambiamos el parámetro de probabilidad de éxito (remoción) en la Binomial? Como en este caso el likelihood depende de un solo parámetro, podemos responder a esta pregunta gráficamente. 

```{r, fig.width = 5, fig.height = 3.5}
op <- par(mfrow=c(1,2),lwd=2, bty="l", las=1)   
theta <- seq(0,1,length=1000)
plot(theta, dbinom(quintral$Removidos[1], size=quintral$Frutos[1], prob=theta, log=F),
     type="l", ylab="L", xlab = expression(theta))
plot(theta,-dbinom(quintral$Removidos[1], size=quintral$Frutos[1], prob=theta, log=T),
     type="l", ylab="-logL", xlab = expression(theta))
par(op)
```

También podemos ver cómo cambia el Likelihood cuando cambiamos el número de observaciones que consideramos. ¿Cómo interpretan estos cambios?

```{r, fig.width = 7, fig.height = 5}
op<-par(mfrow = c(2,3), lwd=2, bty="l", cex.lab=1.5, las = 1, mar = c(5,4,1,1))
theta <- seq(0,1,length=1000)
plot(theta,dbinom(quintral$Removidos[1], size=quintral$Frutos[1], prob=theta,log=F), 
     type="l",xlab="", ylab="L", main="n = 1")

nLL2 <- array(0,length(theta))
for (i in 1:length(theta)) {  
  nLL2[i] <- -sum(dbinom(quintral$Removidos[1:2], size=quintral$Frutos[1:2],
                         prob=theta[i],log=TRUE))
}
plot(theta,exp(-nLL2), type = "l", xlab = "", ylab="", main="n=2")

nLL10 <- array(0,length(theta))
for (i in 1:length(theta)) {
  nLL10[i] <- -sum(dbinom(quintral$Removidos[1:10],size=quintral$Frutos[1:10],
                          prob=theta[i],log=TRUE))
}
plot(theta,exp(-nLL10), type = "l",xlab = "", ylab = "", main="n=10")

plot(theta, -dbinom(quintral$Removidos[1], size=quintral$Frutos[1], prob=theta,log=T), 
     type="l",xlab= "", ylab="-logL", main="")
plot(theta, nLL2, type = "l",xlab=expression(theta), ylab="", main="", ylim=c(0,100))
plot(theta, nLL10, type = "l", xlab="", ylab="", main="", ylim=c(0,100))
par(op)
```

# Buscando el máximo (o más bien el mínimo...)

Queremos encontrar el valor de probabilidad de remoción por fruto que maximiza el likelihood (o minimiza el *negative log-likelihood*) del conjunto de datos. Una opción es hacerlo a lo bruto:
  
```{r}
p <- seq(0, 1, length = 50)  
nLL <- array(NA, length(p))  

for (i in 1:length(p)) {
  nLL[i] <- -sum(dbinom(quintral$Removidos, size=quintral$Frutos, prob=p[i], log=TRUE))
}

p[which(nLL == min(nLL))]
```

En este caso, tenemos una solución analítica para la estimación de la tasa de remoción y podemos compararla con la que acabamos de calcular:
```{r}
sum(quintral$Removidos)/sum(quintral$Frutos)
```

¿Qué pasa si cambiamos la resolución en el vector de probabilidades de remoción (cambiando `length = 50`)?

## Búsquedas más eficientes usando "optim" y "mle2" de [Ben Bolker](http://ms.mcmaster.ca/~bolker/)

Claramente no podemos recurrir a búsquedas gráficas o de fuerza bruta cuando tenemos modelos con varios parámetros. Entonces, tenemos que hacer uso de los algorítmos de búsqueda que están disponibles en `R` con la función `optim`. Primero tenemos que definir una función para "optimizar" que para `optim` viene a ser encontrar el mínimo. Además, tenemos que definir por dónde empezar a buscar y con qué método.

```{r}
binomNLL1 <- function(p,k,N){
  -sum(dbinom(k, prob=p, size=N, log=TRUE)) 
}

O1 <- optim(fn = binomNLL1, par=list(p = 0.5), N=quintral$Frutos, 
            k=quintral$Removidos, method="BFGS")
```

Hay algunos "warnings" de `NAs` producidos en `dbinom` pero nada de qué preocuparse...
El objeto de salida de `optim`, que en este caso se llama `O1` (por falta de creatividad) contiene:

- en `$par`, el valor óptimo del parámetro "p" 
- en `$value`, el valor del log likelihood para $par 
- en `$counts`, hay algunas cosas crípticas acerca de cómo se desarrolló la búsqueda 

Hay que asegurarse de que el algoritmo haya convergido ( `$convergence = 0` )

La función `optim` en `R` es una función general para optimización, pero para MLE nos conviene usar `mle2` desarrollada por Ben Bolker y parte del paquete `bbmle`. En `mle2` la función de negative log-likelihood se llama "minuslogl" en vez de "fn" (en `optim`).

```{r}
library(bbmle) 

m1 <- mle2(minuslogl = binomNLL1, start = list(p = 0.5), 
           data = list(N=quintral$Frutos, k=quintral$Removidos), method = "BFGS")
```

En `m1` tenemos la función usada, los valores de los coeficientes y el log-likelihood estimado. Podemos ver más detalles con `summary(m1)`

```{r}
summary(m1)
```

Ahora tenemos errores standares y valores de 'p' para los coeficientes (poniendo a prueba la hipótesis nula de que el coeficiente es igual a cero). También aparece la devianza (-2 log-likelihood). Más adelante veremos como usar éstas y otras salidas de `mle2` para obtener intervalos de confianza y hacer comparaciones entre modelos. 

## Comparación (cruda) con los datos

Una vez que ajustamos el modelo es natural preguntarse qué tan bueno es el modelo para representar el proceso ecológico que nos interesa. Una forma rústica pero efectiva para hacer esta evaluación es graficar la distibución de valores observados con los predichos por el modelo:

```{r}
k <- quintral$Removidos
hist(k, breaks = 40, freq=FALSE, ylim = c(0,0.15), main = "")
points(0:50,dbinom(0:50,size=round(mean(quintral$Frutos)),prob=m1@coef),
       pch=16,col="darkgray")
```

Un probalema de la comparación que acabamos de hacer es que el número de frutos disponibles es variable. Para hacer una mejor comparación podemos usar un bootstrap (remuestreo)

````{r}
nboot <- 1000
res <- matrix(0,length(quintral$Frutos),nboot)
for (i in 1:nboot){
  n <- sample(quintral$Frutos, length(quintral$Frutos), replace=TRUE)
  res[,i] <- rbinom(length(quintral$Frutos),size = n, prob = m1@coef)
}
hist(k, breaks = 40, freq=F, ylim = c(0,0.15), main = "")
lines(density(res), lwd=2)
```

¿Qué se puede decir sobre el ajuste del modelo a los datos?

## Ejercicio
1. Estimar las probabilidades de remoción de frutos por sitio y tipo de bosque.


