---
  knitrBootstrap::bootstrap_document:
    title: "MLE"
    theme: yeti
    highlight: HighlightJS
    theme.chooser: FALSE
    highlight.chooser: FALSE
    fig_retina: 1
---

```{r, echo=FALSE}
library(knitr)
options(figure_counter = FALSE)
opts_knit$set(eval.after='fig.cap')
knitr::opts_chunk$set(fig.width=5, fig.height=4, tidy=TRUE, results='markup', echo=TRUE, warning=FALSE, message=FALSE, tidy.opts=list(blank=FALSE, width.cutoff=55), dev='svg')
```

#   Máxima Verosimilitud (Likelihood) 

## Objetivos: 
 - Relacionar el "likelihood" de datos con parámetros de una distribución 
 - Familiarizarse con curvas de likelihood y log-likelihood 
 - Comprobar que la forma de la curva de likelihood depende del tamaño muestreal
 - Aprender a usar herramientas para encontrar óptimos

Vimos que las [funciones de densidad](https://es.wikipedia.org/wiki/Función_de_densidad_de_probabilidad) nos dan la probabilidad de observar un valor determinado de una variable aleatoria (para variables continuas nos dan la probabilidad alrededor de un valor). Por ejemplo, para una distribución de Poisson, la probabilidad para un valor $y$ es $\lambda^{y} \exp(- \lambda) / y!$ (donde $!$ es el [factorial](https://es.wikipedia.org/wiki/Factorial) de $y$, no que $y$ sea copado...). Entonces, para una variable aleatoria que asumimos tiene una distribución de Poisson con $\lambda=2$ la probabilidad de observar un cero ($y=0$) es de $2^{0} \exp(-2)/0! = \exp(-2)$. En `R` usamos la función `dpois` para hacer este tipo de cuentas. Para este ejemplo escribimos `dpois(0, lambda=2)`. 

Normalmente tenemos un conjunto de datos, por ejemplo números de renovales por metro cuadrado de muestreo en un bosque: $y = 1, 0, 2, 1, 0, 0, 1, 1, 2, 2$. Si asumimos que estos datos se pueden describir como el resultado de un proceso aleatorio de tipo Poisson, y que además **las observaciones son independientes**, podemos calcular la probabilidad del conjunto de datos como el producto de las probabilidades de las observaciones:

$$
p(y)= \prod_i^n{ \frac{\lambda^{y_i} \exp(- \lambda)} {y_i!} }
$$

Podemos ver que el producto ($\prod$) se hace usándo $i$ como índice para cada observación del set de datos $y$,  desde $i=1$ hasta $i=n$ que es el tamaño de $y$. Esta probabilidad de observar los datos es lo que llamamos *likelihood* o *verosimilitud*. Para hacer la cuenta de arriba tenemos que definir además el parámetro $\lambda$. La probabilidad de los datos depende entonces no sólo de los valores presentes en el set de datos sino también de la distribución utilizada y el valor del o los parámetros de la distribución. De manera más general, la expresión es un **modelo de datos**, es decir una descripción probabilística de como llegar a los datos. Más adelante veremos como combinando procesos determinísticos y estocásticos podemos armar modelos de datos realistas y satisfactorios.

Veamos ahora como calcular la probabilidad de estos datos en `R` asumiendo $\lambda = 1$:

```{r, tidy=TRUE}
y <- c(1, 0, 2, 1, 0, 0, 1, 1, 2, 2)
lambda <- 1
p.y <- dpois(y, lambda=lambda)
p.y 
prod(p.y)
```

Como las probabilidades son números que van entre $0$ y $1$, el producto de varias probabilidades resulta en números muy pequeños. Vemos en este caso que la probabilidad del **conjunto** de datos es muy baja (`5.67e-06`,  donde `e-06` es $10^{-6}$ o $0.000001$) pero esto no debería sorprendernos porque esa es la probabilidad de encontrar exactamente esos datos. Entonces, para evitar problemas numéricos preferimos trabajar con la suma de los logaritmos de las probabilidades, es decir con el logaritmo del *likelihood*. En `R`, podemos usar la opción `log = TRUE` dentro de las funciones de densidad.  

```{r}
y <- c(1, 0, 2, 1, 0, 0, 1, 1, 2, 2)
lambda <- 1
ll.y <- dpois(y, lambda=lambda, log = TRUE)
sum(ll.y)
```

Una vez que logramos hacer estas cuentas, podemos preguntarnos cómo cambia la probabilidad de observar el set de datos si cambiamos el valor de $\lambda$. Probemos con $\lambda=2$

```{r}
y <- c(1, 0, 2, 1, 0, 0, 1, 1, 2, 2)
lambda <- 2
sum(dpois(y, lambda=lambda, log = TRUE))
```

Con un $\lambda$ de $2$ la *log likelihood* de los datos empeora, pero podemos seguir probando distintos valores de $\lambda$ hasta encontrar el que hace más posible al set de datos. Esta es la idea detrás de los análisis de máxima verosimilitud (a falta de mejor nombre) o de *maximum likelihood* (si estamos dispuestos a usar palabras en inglés). Normalmente tenemos más de un parámetro en nuestro modelo de datos, y usamos aglún algoritmo de búsqueda para encontrar la combinación de parámetros que hace más probables a nuestros datos (por tradición, los algoritmos buscan minimizar el menos logaritmo de la verosimilitud de nuestros datos). Esta idea de buscar combinaciones de parámetros que maximizan la verosimilitud de los datos se conoce como *maximum likelihood estimation* y muchas veces aparece abreviado como MLE.

## Ejemplo: Remoción de Frutos

[Mariano Rodriguez-Cabal](https://sites.google.com/site/rodriguezcabal/home) estudió la remoción de frutos de quintral por parte del monito del monte en bosque continuo y fragmentos en 3 sitios pareados con la idea de ver si la fragmentación afectabla la interacción entre este marsupial y el muérdago. El trabajo  está dispoible [aquí](http://sites.google.com/site/modelosydatos/Rodriguez-Cabal_07.pdf)
 
Los datos son de número de frutos removidos de un total disponible. Podemos usar una distribución Binomial para capturar la variabilidad (estocasticidad) en el proceso de remoción de frutos. Es decir, asumimos que la distribución Binomial tiene sentido para representar el proceso que genera los datos.
 
```{r}
url <- "https://github.com/jmmorales/cursoMyD/raw/master/Data/quintral.txt"
quintral <- read.table(url, header = TRUE)
```

Para ver como están organizados los datos podemos hacer:
```{r}
str(quintral)  
```
Vemos que se trata de un *data frame* que es la manera preferida de `R` para organizar datos. Los data frames tienen a las observaciones en filas y a las variables en columnas. Las variables individuales se pueden acceder escribiendo el nombre del data frame seguido del signo $ y el nombre de la variable. Por ejemplo, si queremos ver los valores de frutos disponibles en las tres primeras observaciones hacemos:

```{r}
quintral$Frutos[1:3]
```

Ahora calculemos el likelihood para la primera observación del set de datos asumiendo que la probabilidad de remoción es $0.5$ (si no nos acordamos cómo es la distribución Binomial o cómo usarla en `R` podemos pedir ayuda con `?dbinom`)

```{r}
dbinom(quintral$Removidos[1],size=quintral$Frutos[1],prob=0.5)
```

Normalmente trabajamos con log-likelihoods:
```{r}
dbinom(quintral$Removidos[1],size=quintral$Frutos[1],prob=0.5,log=TRUE)
```

¿Cómo cambia la probabilidad de observar ese dato cuando cambiamos el parámetro de probabilidad de éxito (remoción) en la Binomial? Como en este caso el likelihood depende de un solo parámetro, podemos responder a esta pregunta gráficamente. 

```{r, fig.width = 5, fig.height = 3.5}
op <- par(mfrow=c(1,2),lwd=2, bty="l", las=1)   
theta <- seq(0,1,length=1000)
plot(theta, dbinom(quintral$Removidos[1], size=quintral$Frutos[1], prob=theta, log=F),
     type="l", ylab="L", xlab = expression(theta))
plot(theta,-dbinom(quintral$Removidos[1], size=quintral$Frutos[1], prob=theta, log=T),
     type="l", ylab="-logL", xlab = expression(theta))
par(op)
```

También podemos ver cómo cambia el Likelihood cuando cambiamos el número de observaciones que consideramos. ¿Cómo interpretan estos cambios?

```{r, fig.width = 7, fig.height = 5}
op<-par(mfrow = c(2,3), lwd=2, bty="l", cex.lab=1.5, las = 1, mar = c(5,4,1,1))
theta <- seq(0,1,length=1000)
plot(theta,dbinom(quintral$Removidos[1], size=quintral$Frutos[1], prob=theta,log=F), 
     type="l",xlab="", ylab="L", main="n = 1")

nLL2 <- array(0,length(theta))
for (i in 1:length(theta)) {  
  nLL2[i] <- -sum(dbinom(quintral$Removidos[1:2], size=quintral$Frutos[1:2],
                         prob=theta[i],log=TRUE))
}
plot(theta,exp(-nLL2), type = "l", xlab = "", ylab="", main="n=2")

nLL10 <- array(0,length(theta))
for (i in 1:length(theta)) {
  nLL10[i] <- -sum(dbinom(quintral$Removidos[1:10],size=quintral$Frutos[1:10],
                          prob=theta[i],log=TRUE))
}
plot(theta,exp(-nLL10), type = "l",xlab = "", ylab = "", main="n=10")

plot(theta, -dbinom(quintral$Removidos[1], size=quintral$Frutos[1], prob=theta,log=T), 
     type="l",xlab= "", ylab="-logL", main="")
plot(theta, nLL2, type = "l",xlab=expression(theta), ylab="", main="", ylim=c(0,100))
plot(theta, nLL10, type = "l", xlab="", ylab="", main="", ylim=c(0,100))
par(op)
```

# Buscando el máximo (o más bien el mínimo...)

Queremos encontrar el valor de probabilidad de remoción por fruto que maximiza el likelihood (o minimiza el *negative log-likelihood*) del conjunto de datos. Una opción es hacerlo a lo bruto:
  
```{r}
p <- seq(0, 1, length = 50)  
nLL <- array(NA, length(p))  

for (i in 1:length(p)) {
  nLL[i] <- -sum(dbinom(quintral$Removidos, size=quintral$Frutos, prob=p[i], log=TRUE))
}

p[which(nLL == min(nLL))]
```

En este caso, tenemos una solución analítica para la estimación de la tasa de remoción y podemos compararla con la que acabamos de calcular:
```{r}
sum(quintral$Removidos)/sum(quintral$Frutos)
```

¿Qué pasa si cambiamos la resolución en el vector de probabilidades de remoción (cambiando `length = 50`)?

## Búsquedas más eficientes usando "optim" y "mle2" de [Ben Bolker](http://ms.mcmaster.ca/~bolker/)

Claramente no podemos recurrir a búsquedas gráficas o de fuerza bruta cuando tenemos modelos con varios parámetros. Entonces, tenemos que hacer uso de los algorítmos de búsqueda que están disponibles en `R` con la función `optim`. Primero tenemos que definir una función para "optimizar" que para `optim` viene a ser encontrar el mínimo. Además, tenemos que definir por dónde empezar a buscar y con qué método.

```{r}
binomNLL1 <- function(p,k,N){
  -sum(dbinom(k, prob=p, size=N, log=TRUE)) 
}

O1 <- optim(fn=binomNLL1, par=list(p = 0.5), N=quintral$Frutos, k=quintral$Removidos, method="BFGS")
```

Hay algunos "warnings" de `NAs` producidos en `dbinom` pero nada de qué preocuparse...
El objeto de salida de `optim`, que en este caso se llama `O1` (por falta de creatividad) contiene:

- en `$par`, el valor óptimo del parámetro "p" 
- en `$value`, el valor del log likelihood para $par 
- en `$counts`, hay algunas cosas crípticas acerca de cómo se desarrolló la búsqueda 

Hay que asegurarse de que el algoritmo haya convergido ( `$convergence = 0` )

La función `optim` en `R` es una función general para optimización, pero para MLE nos conviene usar `mle2` desarrollada por Ben Bolker y parte del paquete `bbmle`. En `mle2` la función de negative log-likelihood se llama "minuslogl" en vez de "fn" (en `optim`).

```{r}
library(bbmle) 

m1 <- mle2(minuslogl=binomNLL1, start = list(p = 0.5), data = list(N=quintral$Frutos, k=quintral$Removidos), method="BFGS")
```

En `m1` tenemos la función usada, los valores de los coeficientes y el log-likelihood estimado. Podemos ver más detalles con `summary(m1)`

```{r}
summary(m1)
```

Ahora tenemos errores standares y valores de 'p' para los coeficientes (poniendo a prueba la hipótesis nula de que el coeficiente es igual a cero). También aparece la devianza (-2 log-likelihood). Más adelante veremos como usar éstas y otras salidas de `mle2` para obtener intervalos de confianza y hacer comparaciones entre modelos. 

## Comparación (cruda) con los datos

Una vez que ajustamos el modelo es natural preguntarse qué tan bueno es el modelo para representar el proceso ecológico que nos interesa. Una forma rústica pero efectiva para hacer esta evaluación es graficar la distibución de valores observados con los predichos por el modelo:

```{r}
k <- quintral$Removidos
hist(k, breaks = 40, freq=FALSE, ylim = c(0,0.15), main = "")
points(0:50,dbinom(0:50,size=round(mean(quintral$Frutos)),prob=m1@coef),
       pch=16,col="darkgray")
```

Un probalema de la comparación que acabamos de hacer es que el número de frutos disponibles es variable. Para hacer una mejor comparación podemos usar un bootstrap (remuestreo)

```{r}
nboot <- 1000
res <- matrix(0,length(quintral$Frutos),nboot)
for (i in 1:nboot){
  n <- sample(quintral$Frutos, length(quintral$Frutos), replace=TRUE)
  res[,i] <- rbinom(length(quintral$Frutos),size = n, prob = m1@coef)
}
hist(k, breaks = 40, freq=F, ylim = c(0,0.15), main = "")
lines(density(res), lwd=2)
```

Una alternativa más estándar es graficar los residuos ($y - \hat{y}$):
```{r}
plot(m1@coef * quintral$Frutos - quintral$Removidos, xlab = "", ylab = "Residuos")
abline(h=0, lwd=2, lty=2)

```



¿Qué se puede decir sobre el ajuste del modelo a los datos?

## Ejercicio
1. Estimar las probabilidades de remoción de frutos por sitio y tipo de bosque.

-------------------------

## Ejemplo: Deposición de polen en flores de pomelo 

[Natacha Chacoff](https://scholar.google.com/citations?user=rxJBqGcAAAAJ&hl=en) estudió la deposición depolen en flores de pomelo en función del número de visitas de abejas. El trabajo terminado se puede ver [aquí](https://sites.google.com/site/modelosydatos/Chacoff_2008.pdf) 


```{r}
apis <- read.table("https://sites.google.com/site/modelosydatos/polen_Apis.csv",header=T,sep=",")

# Graficar los datos (plot) 
plot(apis$visitas, apis$granos, xlab="Número de Visitas", 
     ylab="Granos de Polen Depositados", pch=19)
title(main= expression(paste("Eficiencia de ", italic(Apis~mellifera))))
```

Podemos tratar de descibir esta relación entre visitas y deposición de polen como una función lineal asumiendo una distribución de Poisson para los granos de polen.

```{r}
# definir la función de negative log-Likelihood 
fnLIN<-function (p) {
 y0 <- p[1]
 a <- p[2]
 lambda <- y0 + a * visitas   # qué problemas podemos tener con esta fórmula?
 -sum(dpois(granos, lambda, log=TRUE))
}

# definimos nombres para los parámetros 
parnames(fnLIN) <- c("y0", "a")

# ajustamos a los datos
mLIN <- mle2(fnLIN, start = c(y0 = 10, a = 3), 
             data = list(visitas = apis$visitas, granos = apis$granos))

# ver resultados
summary(mLIN)

# agregamos el modelo ajustado al gráfico anterior
plot(apis$visitas, apis$granos,xlab="Número de Visitas", 
     ylab="Granos de Polen Depositados", pch=19)
title(main= expression(paste("Eficiencia de ", italic(Apis~mellifera))))
x = 0:9 # vector de valores de referencia
lines(x, mLIN@coef[1] + x * mLIN@coef[2], lwd=2)
```

Alternativamente podemos pensar que hay un efecto de saturación:

```{r}
# neg log L
fnSAT <- function (p) {
 y0 <- p[1]
 a <- p[2]
 b <- p[3]
 lambda <- y0 + a*(1-exp(-b*visitas))  # qué otro modelo de saturación podríamos usar?
 -sum(dpois(granos, lambda, log=TRUE))
}

parnames(fnSAT) <- c("y0", "a", "b")
mSAT <- mle2(fnSAT, start = c(y0 = 10, a = 41, b = 0.16), 
             data = list(visitas = apis$visitas, granos = apis$granos))

summary(mSAT)
plot(apis$visitas, apis$granos,xlab="Número de Visitas", 
     ylab="Granos de Polen Depositados", pch=19)
title(main= expression(paste("Eficiencia de ", italic(Apis~mellifera))))
lines(x, mSAT@coef[1] + mSAT@coef[2]*(1-exp(-mSAT@coef[3]* x)), lwd=2, col=2)
```

¿Cómo ajustan estos modelos a los datos?
¿Qué otros modelos podríamos probar? 

# Aves carroñeras y rutas 
Lambertucci y colaboradores pusieron cadáveres de ovejas a distintas distancias de rutas y observaron la actividad de aves carroñeras. Estudian la detección y el uso de carroña en relación a la distancia a las rutas por cóndores y otras aves rapaces. El trabajo completo está en https://sites.google.com/site/modelosydatos/Lambertucci_09.pdf y los datos los pueden bajar de https://sites.google.com/site/pajarom/condor.txt

Estos datos tienen exeso de ceros, entonces necesitamos escribir una función para la regresión binomial "inflada" en ceros. Para el total de aves observadas en cada sitio de carroña (volando y en tierra) podemos asumir una Poisson con exeso de ceros. Para la fracción de aves que bajan hasta la carroña asumimos una Binomial.

Además, vamos a usar la función ```plogis``` para pasar de valores reales (positivos y negativos) a valores entre cero y uno. Para ver como funciona este truco hacemos:
```{r}
tmp <- seq(from=-5, to=5, by=0.01) 
plot(tmp,plogis(tmp),type="l", lwd=2)
```

De esta forma, podemos usar covariables para la probabilidad de éxito de una binomial.
Todo junto queda:

```{r, fig.width = 7, fig.height = 8}
condor <- read.table("https://sites.google.com/site/modelosydatos/condor.txt", header=T)
head(condor) # revisamos los datos
                                                     
zibinregc = function(p) {
  av = p[1] # probabilidad (sin transformar) de detectar la carroña
  ad = p[2] # ordenada al origen de bajar a comer
  bd = p[3] # la pendiente de bajar a comer 
  an = p[4] # ordenada al origen de abundancia 
  bn = p[5] # pendiente de la distancia a la ruta
  
  v = plogis(av)  #  probabilidad de detectar una carroña 
  pr = plogis(ad + bd * condor$Dist/1000) # probabilidad de bajar a comer
  lambda = exp(an + bn * condor$Dist/1000) # Poisson en función de la distancia a la ruta
  nllp = -sum(log(ifelse(k+f==0,(1-v) + v*dpois(0,lambda), v*dpois(k+f,lambda))))  
  # Poisson likelihood inflada en ceros
  nllb = -sum(dbinom(k,k+f,prob=pr,log=T))   # likelihood Binomial
  return(nllp+nllb) # el resultado es la suma de las dos negative log-likelihoods
}

# definir los nombres de los parámetros para que los use mle2
parnames(zibinregc) = c("av","ad","bd","an","bn")  

#Ajustamos la funcion para los cóndores
fit.C <- mle2(minuslogl=zibinregc, start=c(av=1, ad=2, bd=0, an=2, bn=0), 
              data=list(k=condor$Condor,f=condor$CoFl, Dist=condor$Dist))  

summary(fit.C) # ver los resultados

par_cof <- fit.C@coef # extraer los coeficientes (MLE de los parámetros)
d = 0:6000   # variable para graficar distancia

# gráfico de los ajustes
op=par(mfrow=c(2,1), lwd=2)                             
plot(condor$Dist,(condor$Condor+condor$CoFl), ylab = "# de Cóndores", 
     xlab="Distancia a la Ruta")                                    
lines(d,exp(par_cof[4]+par_cof[5]*d/1000))
plot(condor$Dist, condor$Condor/(condor$Condor+condor$CoFl),
     ylab="Proporción de Cóndores comiendo", xlab="Distancia a la Ruta")
lines(d,plogis(par_cof[2]+par_cof[3]*d/1000))
par(op)             

# Lo mismo para Jote negro 
fit.J <- mle2(zibinregc, start = c(av=1, ad=2, bd=0, an=2, bn=0), 
              data = list(k=condor$JoteN,f=condor$JNFl, Dist=condor$Dist))  

summary(fit.J)
par_cof <- fit.J@coef

op=par(mfrow=c(2,1), lwd=2)                             
plot(condor$Dist,(condor[,11]+condor[,12]), ylab = "# de Jote Negro", 
     xlab="Distancia a la Ruta")                                    
lines(d,exp(par_cof[4]+par_cof[5]*d/1000))
plot(condor$Dist, condor[,11]/(condor[,11]+condor[,12]),
     ylab="Proporción de Jote Negro comiendo", xlab="Distancia a la Ruta")
lines(d,plogis(par_cof[2]+par_cof[3]*d/1000))
par(op) 
```

#---------------

```{r}
sessionInfo()
```



